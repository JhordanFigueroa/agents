{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TOOL CALLING WITH AGENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINE TOOLS\n",
    "def add(x: int, y: int) -> int:\n",
    "    \"\"\"Add two numbers together\"\"\"\n",
    "    return x + y\n",
    "\n",
    "def substract(x: int, y: int) -> int:\n",
    "    \"\"\"Substract two numbers\"\"\"\n",
    "    return x - y\n",
    "\n",
    "def multiply(x: int, y: int) -> int:\n",
    "    \"\"\"Multiply two numbers\"\"\"\n",
    "    return x * y\n",
    "\n",
    "def divide(x: int, y: int) -> int:\n",
    "    \"\"\"Divide two numbers\"\"\"\n",
    "    return x / y\n",
    "\n",
    "def get_user_info(username: str) -> str:\n",
    "    \"\"\"Get user information\"\"\"\n",
    "\n",
    "    database = {\n",
    "        \"Antonio\": {\n",
    "            \"name\": \"Antonio Lopez\",\n",
    "            \"age\": 30,\n",
    "            \"email\": \"antonio@example.com\"\n",
    "        }, \n",
    "        \"Nelson\": {\n",
    "            \"name\": \"Nelson Rodriguez\",\n",
    "            \"age\": 25,\n",
    "            \"email\": \"nelson@example.com\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return f\"Username: {username}, Info: {database.get(username.lower(), 'User not found')}\"\n",
    "\n",
    "\n",
    "#CREATE TOOLS FROM PYTHON FUNCTIONS \n",
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "addition_tools = FunctionTool.from_defaults(fn=add)\n",
    "substraction_tools = FunctionTool.from_defaults(fn=substract)\n",
    "multiplication_tools = FunctionTool.from_defaults(fn=multiply)\n",
    "divide_tools = FunctionTool.from_defaults(fn=divide)\n",
    "get_user_info_tools = FunctionTool.from_defaults(fn=get_user_info)\n",
    "\n",
    "tools = [addition_tools, substraction_tools, multiplication_tools, divide_tools, get_user_info_tools]\n",
    "\n",
    "#TEST TOOLS\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = llm.predict_and_call(\n",
    "#     tools,\n",
    "#     \"Add 2 and 3\", \n",
    "#     verbose=True\n",
    "# )\n",
    "\n",
    "# print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VECTOR SEARCH WITH METADATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "#read paper\n",
    "documents = SimpleDirectoryReader(input_files=[\"./datasets/lora_paper.pdf\"]).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "nodes = splitter.get_nodes_from_documents(documents=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###LLM AND EMBEDDING MODEL\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "Settings.embedding = OpenAIEmbedding(model=\"text-embedding-ada-002\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "vector_index = VectorStoreIndex(nodes=nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.vector_stores import MetadataFilters\n",
    "\n",
    "query_engine = vector_index.as_query_engine(\n",
    "    similarity_top_k=3,\n",
    "    filters=MetadataFilters.from_dicts(\n",
    "        [\n",
    "            {\"key\": \"page_label\", \"value\": \"2\"}\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "# response = query_engine.query(\"Tell me about the problem statement as explained in page 2\")\n",
    "# print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for n in response.source_nodes:\n",
    "#     print(n.metadata)\n",
    "#     print(\"==================\")\n",
    "#     print(n.get_text())\n",
    "#     print(\"==================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from llama_index.core.vector_stores import FilterCondition\n",
    "from llama_index.core.vector_stores import MetadataFilters\n",
    "\n",
    "def vector_search_query(\n",
    "        query: str, \n",
    "        page_numbers: List[str]\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Conduct a vector search across an index using the following parameters:\n",
    "\n",
    "    query (str): This is the text string you want to embed and search for within the index.\n",
    "    page_numbers (List[str]): This parameter allows you to limit the search to \n",
    "    specific pages. If left empty, the search will encompass all pages in the index. \n",
    "    If page numbers are specified, the search will be filtered to only include those pages.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    metadata_dicts = [\n",
    "        {\"key\": \"page_label\", \"value\": p} for p in page_numbers\n",
    "    ]\n",
    "\n",
    "    query_engine = vector_index.as_query_engine(\n",
    "        similarity_top_k=2,\n",
    "        filters=MetadataFilters.from_dicts(\n",
    "            metadata_dicts,\n",
    "            condition=FilterCondition.OR\n",
    "        )\n",
    "    )\n",
    "\n",
    "    response = query_engine.query(query)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_query_tool = FunctionTool.from_defaults(\n",
    "    fn=vector_search_query,\n",
    "    name=\"vector_search_query_tool\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response=llm.predict_and_call(\n",
    "#     [vector_query_tool],\n",
    "#     \"Explain the problem statement in page 2\",\n",
    "#     verbose=True\n",
    "\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for n in response.source_nodes:\n",
    "#     print(n.metadata)\n",
    "#     print(\"==================\")\n",
    "#     print(n.get_text())\n",
    "#     print(\"==================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SummaryIndex\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "summary_index = SummaryIndex(nodes=nodes)\n",
    "\n",
    "summary_query_engine_tool = summary_index.as_query_engine(\n",
    "    use_async=True,\n",
    "    response_mode=\"tree_summarize\"\n",
    ")\n",
    "\n",
    "summary_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=summary_query_engine_tool,\n",
    "    name=\"summary_tool\",\n",
    "    description=\"Useful for summarization questions related to the Lora paper.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = llm.predict_and_call(\n",
    "#     [summary_tool, vector_query_tool],\n",
    "#     \"Summarize how to apply Lora to Transfomer in page 5 in 2 sentences\",\n",
    "#     verbose=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AGENT WORKER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    tools=[summary_tool, vector_query_tool], \n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Explain low-rank structures in Deep Learning and how it applies  for llms\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool with args: {\"input\": \"Explain low-rank structures in Deep Learning and how it applies for LLMs\"}\n",
      "=== Function Output ===\n",
      "Low-rank structures in Deep Learning involve representing weight matrices in neural networks using a reduced number of dimensions. This approach helps in decreasing computational complexity and memory requirements while retaining crucial information. In the context of Large Language Models (LLMs) like GPT-2 and GPT-3, low-rank structures are employed to efficiently adapt pre-trained models to specific downstream tasks. By updating only a low-rank approximation of the weight matrices during fine-tuning, LLMs can be effectively adapted to new tasks without significantly increasing the number of trainable parameters. This method enables the adaptation of LLMs to various tasks while maintaining performance and reducing computational overhead.\n",
      "=== LLM Response ===\n",
      "Low-rank structures in Deep Learning involve representing weight matrices in neural networks using a reduced number of dimensions. This approach helps in decreasing computational complexity and memory requirements while retaining crucial information. In the context of Large Language Models (LLMs) like GPT-2 and GPT-3, low-rank structures are employed to efficiently adapt pre-trained models to specific downstream tasks. By updating only a low-rank approximation of the weight matrices during fine-tuning, LLMs can be effectively adapted to new tasks without significantly increasing the number of trainable parameters. This method enables the adaptation of LLMs to various tasks while maintaining performance and reducing computational overhead.\n",
      "Low-rank structures in Deep Learning involve representing weight matrices in neural networks using a reduced number of dimensions. This approach helps in decreasing computational complexity and memory requirements while retaining crucial information. In the context of Large Language Models (LLMs) like GPT-2 and GPT-3, low-rank structures are employed to efficiently adapt pre-trained models to specific downstream tasks. By updating only a low-rank approximation of the weight matrices during fine-tuning, LLMs can be effectively adapted to new tasks without significantly increasing the number of trainable parameters. This method enables the adaptation of LLMs to various tasks while maintaining performance and reducing computational overhead.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Explain low-rank structures in Deep Learning and how it applies  for llms\"\n",
    ")\n",
    "\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Explain what is LoRA and why and how it's used. Are exisiting solutions not enough?\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool with args: {\"input\": \"Explain what is LoRA and why and how it's used. Are existing solutions not enough?\"}\n",
      "=== Function Output ===\n",
      "LoRA, or Low-Rank Adaptation, is a method used to adapt large-scale pre-trained language models to specific tasks or domains. It involves freezing the pre-trained model weights and introducing trainable rank decomposition matrices into each layer of the Transformer architecture. This approach significantly reduces the number of trainable parameters for downstream tasks, making training more efficient. LoRA allows for optimizing the rank decomposition matrices of dense layers during adaptation while keeping the pre-trained weights frozen. When deployed, LoRA does not introduce additional inference latency compared to fully fine-tuned models.\n",
      "\n",
      "Existing solutions for adapting language models, such as adapter layers or optimizing input layer activations, have limitations. Adapter layers can lead to inference latency, while optimizing input layers can be challenging to optimize and may reduce the available sequence length for downstream tasks. LoRA addresses these limitations by providing a parameter-efficient approach that maintains model quality, reduces the number of trainable parameters, and efficiently fine-tunes models for new tasks by emphasizing important features without overfitting.\n",
      "=== LLM Response ===\n",
      "LoRA, or Low-Rank Adaptation, is a method used to adapt large-scale pre-trained language models to specific tasks or domains. It involves freezing the pre-trained model weights and introducing trainable rank decomposition matrices into each layer of the Transformer architecture. This approach significantly reduces the number of trainable parameters for downstream tasks, making training more efficient. LoRA allows for optimizing the rank decomposition matrices of dense layers during adaptation while keeping the pre-trained weights frozen. When deployed, LoRA does not introduce additional inference latency compared to fully fine-tuned models.\n",
      "\n",
      "Existing solutions for adapting language models, such as adapter layers or optimizing input layer activations, have limitations. Adapter layers can lead to inference latency, while optimizing input layers can be challenging to optimize and may reduce the available sequence length for downstream tasks. LoRA addresses these limitations by providing a parameter-efficient approach that maintains model quality, reduces the number of trainable parameters, and efficiently fine-tunes models for new tasks by emphasizing important features without overfitting.\n",
      "LoRA, or Low-Rank Adaptation, is a method used to adapt large-scale pre-trained language models to specific tasks or domains. It involves freezing the pre-trained model weights and introducing trainable rank decomposition matrices into each layer of the Transformer architecture. This approach significantly reduces the number of trainable parameters for downstream tasks, making training more efficient. LoRA allows for optimizing the rank decomposition matrices of dense layers during adaptation while keeping the pre-trained weights frozen. When deployed, LoRA does not introduce additional inference latency compared to fully fine-tuned models.\n",
      "\n",
      "Existing solutions for adapting language models, such as adapter layers or optimizing input layer activations, have limitations. Adapter layers can lead to inference latency, while optimizing input layers can be challenging to optimize and may reduce the available sequence length for downstream tasks. LoRA addresses these limitations by providing a parameter-efficient approach that maintains model quality, reduces the number of trainable parameters, and efficiently fine-tunes models for new tasks by emphasizing important features without overfitting.\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\n",
    "    \"Explain what is LoRA and why and how it's used. Are exisiting solutions not enough?\",\n",
    ")\n",
    "\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    tools=[summary_tool, vector_query_tool], \n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = agent.create_task(\n",
    "    \"Explain what is LoRA and why and how it's used.\"\n",
    "    \"Are existing solutions not enough?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Summarize the LoRA paper\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool with args: {\"input\": \"LoRA paper\"}\n",
      "=== Function Output ===\n",
      "The LoRA paper introduces a method for efficient adaptation of large language models by freezing pre-trained model weights and incorporating trainable rank decomposition matrices into each layer of the Transformer architecture. This approach reduces the number of trainable parameters for downstream tasks while maintaining or even surpassing the performance of traditional fine-tuning methods. The paper includes empirical investigations into rank-deficiency during language model adaptation and demonstrates the efficacy of LoRA through experiments on tasks like E2E NLG Challenge, MNLI, and WikiSQL. Additionally, the paper discusses the Backward Feature Correction method and its application in deep learning tasks, highlighting the benefits of LoRA modules with varying ranks in adapting pre-trained models such as GPT-2 and GPT-3.\n"
     ]
    }
   ],
   "source": [
    "step_output = agent.run_step(\n",
    "    task.task_id, \n",
    "    input = \"Summarize the LoRA paper\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LLM Response ===\n",
      "The LoRA paper introduces a method for efficient adaptation of large language models by incorporating trainable rank decomposition matrices into each layer of the Transformer architecture. This approach reduces the number of trainable parameters for downstream tasks while maintaining or surpassing performance. The paper includes empirical investigations and experiments on tasks like E2E NLG Challenge, MNLI, and WikiSQL, demonstrating the efficacy of LoRA. Additionally, it discusses the Backward Feature Correction method and the benefits of LoRA modules with varying ranks in adapting pre-trained models like GPT-2 and GPT-3.\n"
     ]
    }
   ],
   "source": [
    "step_output = agent.run_step(task.task_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fa2869ea-cf7a-471b-962f-97b898bb9a82\n",
      "fa2869ea-cf7a-471b-962f-97b898bb9a82\n",
      "The LoRA paper introduces a method for efficient adaptation of large language models by freezing pre-trained model weights and incorporating trainable rank decomposition matrices into each layer of the Transformer architecture. This approach reduces the number of trainable parameters for downstream tasks while maintaining or even surpassing the performance of traditional fine-tuning methods. The paper includes empirical investigations into rank-deficiency during language model adaptation and demonstrates the efficacy of LoRA through experiments on tasks like E2E NLG Challenge, MNLI, and WikiSQL. Additionally, the paper discusses the Backward Feature Correction method and its application in deep learning tasks, highlighting the benefits of LoRA modules with varying ranks in adapting pre-trained models such as GPT-2 and GPT-3.\n"
     ]
    }
   ],
   "source": [
    "completed_step = agent.get_completed_steps(task.task_id)\n",
    "\n",
    "print(task.task_id)\n",
    "if len(completed_step) > 0:\n",
    "    print(task.task_id)\n",
    "    print(completed_step[0].output.sources[0].raw_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(step_output.is_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "upcoming_steps = agent.get_upcoming_steps(task.task_id)\n",
    "\n",
    "if len(upcoming_steps) > 0:\n",
    "    print(upcoming_steps[0].input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step_output = agent.run_step(task.task_id)\n",
    "# print(step_output.is_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA is a method used to adapt pre-trained language models for downstream tasks by injecting trainable rank decomposition matrices into each layer of the Transformer architecture. This approach reduces the number of trainable parameters while maintaining or improving model quality compared to traditional fine-tuning methods. LoRA amplifies task-specific directions in the weight matrices of pre-trained models by updating a low-rank matrix to emphasize important but not emphasized directions in the original weight matrix. The amplification factor in LoRA varies based on the chosen rank for the adaptation matrix, with lower ranks generally amplifying task-specific directions more effectively.\n",
      "\n",
      "As for why and how LoRA is used, it offers a more efficient way to adapt pre-trained language models for specific tasks by enhancing task-specific information in the model while reducing the number of parameters that need to be fine-tuned. This can lead to improved performance and faster adaptation to new tasks.\n",
      "\n",
      "Existing solutions may not be enough because traditional fine-tuning methods can be computationally expensive and may require a large number of parameters to be updated, which can lead to overfitting or suboptimal performance. LoRA addresses these challenges by providing a more efficient and effective way to adapt pre-trained models for downstream tasks.\n"
     ]
    }
   ],
   "source": [
    "response = agent.finalize_response(task.task_id)\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (agenv)",
   "language": "python",
   "name": "agenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
