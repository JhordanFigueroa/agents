{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = [\n",
    "    \"./datasets/lora_paper.pdf\", \n",
    "    \"./datasets/longlora_efficient_fine_tuning.pdf\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_doc_tools\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating ./datasets/lora_paper.pdf tool\n",
      "Creating ./datasets/longlora_efficient_fine_tuning.pdf tool\n"
     ]
    }
   ],
   "source": [
    "paper_to_tools_dict = {}\n",
    "\n",
    "for paper in papers:\n",
    "    print(f\"Creating {paper} tool\")\n",
    "    path = Path(paper)\n",
    "    vector_tool, summary_tool = await create_doc_tools(doc_name=path.stem, document_fp=path)\n",
    "    paper_to_tools_dict[path.stem] = [vector_tool, summary_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lora_paper': [<llama_index.core.tools.query_engine.QueryEngineTool at 0x1377db770>,\n",
       "  <llama_index.core.tools.query_engine.QueryEngineTool at 0x127d42ea0>],\n",
       " 'longlora_efficient_fine_tuning': [<llama_index.core.tools.query_engine.QueryEngineTool at 0x137b85850>,\n",
       "  <llama_index.core.tools.query_engine.QueryEngineTool at 0x144ae3b90>]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_to_tools_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<llama_index.core.tools.query_engine.QueryEngineTool object at 0x1377db770>, <llama_index.core.tools.query_engine.QueryEngineTool object at 0x127d42ea0>, <llama_index.core.tools.query_engine.QueryEngineTool object at 0x137b85850>, <llama_index.core.tools.query_engine.QueryEngineTool object at 0x144ae3b90>]\n"
     ]
    }
   ],
   "source": [
    "initial_tools = [t for paper in papers for t in paper_to_tools_dict[Path(paper).stem]]\n",
    "print(initial_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(initial_tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AGENT WORKER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    initial_tools,\n",
    "    llm=llm,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Explain what is Lora and how I can use it in my applications and saves me money?Explain what is LongLora and why it's being used.Compare LongLora and LoRA. Which one is better?\n",
      "=== Calling Function ===\n",
      "Calling function: lora_paper_summary_query_engine_tool with args: {\"input\": \"Explain what is Lora and how I can use it in my applications and saves me money?\"}\n",
      "=== Function Output ===\n",
      "LoRA, or Low-Rank Adaptation, is a method used to adapt large-scale pre-trained language models to specific tasks or domains by injecting trainable rank decomposition matrices into each layer of the Transformer architecture. By implementing LoRA in your applications, you can efficiently switch between different tasks by replacing the matrices A and B, reducing storage requirements and task-switching overhead significantly. This allows you to share a pre-trained model and build multiple small LoRA modules for various tasks, reducing the need to store and load independent instances of fine-tuned models. Ultimately, integrating LoRA into your workflow can lead to cost savings by reducing GPU memory requirements, training costs, and the need for extensive retraining when adapting pre-trained models to new tasks.\n",
      "=== Calling Function ===\n",
      "Calling function: longlora_efficient_fine_tuning_summary_query_engine_tool with args: {\"input\": \"Explain what is LongLora and why it's being used.\"}\n",
      "=== Function Output ===\n",
      "LongLora is a framework that extends the context length of large language models efficiently while minimizing accuracy compromise. It incorporates the S2-Attn mechanism during training to approximate standard self-attention patterns, while retaining the original attention architecture during inference for reusability. LongLora bridges the gap between LoRA and full fine-tuning by incorporating trainable normalization and embedding layers. Additionally, LongLora includes components like the Action Units Relation Transformer (ART) and Tampered AU Prediction (TAP) to model intra-face relations and enhance deep fake detection model generalization. It is used to improve forgery detection by capturing intra-face relations and providing local tampering supervision, ultimately achieving state-of-the-art performance on various evaluations.\n",
      "=== Calling Function ===\n",
      "Calling function: lora_paper_vector_query_engine_tool with args: {\"input\": \"Compare LongLora and LoRA. Which one is better?\"}\n",
      "=== Function Output ===\n",
      "LongLora is not directly mentioned in the provided context information.\n",
      "=== Calling Function ===\n",
      "Calling function: longlora_efficient_fine_tuning_vector_query_engine_tool with args: {\"input\": \"Compare LongLora and LoRA. Which one is better?\"}\n",
      "=== Function Output ===\n",
      "LongLoRA is better than LoRA.\n",
      "=== LLM Response ===\n",
      "Based on the information retrieved:\n",
      "\n",
      "1. **LoRA** (Low-Rank Adaptation) is a method used to adapt large-scale pre-trained language models to specific tasks by injecting trainable rank decomposition matrices into each layer of the Transformer architecture. By implementing LoRA in your applications, you can efficiently switch between different tasks, reduce storage requirements, and save costs by reducing GPU memory requirements and training costs.\n",
      "\n",
      "2. **LongLora** is a framework that extends the context length of large language models efficiently while minimizing accuracy compromise. It incorporates the S2-Attn mechanism during training to approximate standard self-attention patterns and includes components like the Action Units Relation Transformer (ART) and Tampered AU Prediction (TAP) to enhance deep fake detection model generalization.\n",
      "\n",
      "3. **Comparison**: Based on the limited information available, it seems that **LongLora** is considered better than **LoRA**. However, further details on the comparison between LongLora and LoRA are not provided in the retrieved context.\n",
      "\n",
      "If you need more detailed information or have any specific questions, feel free to ask!\n",
      "Based on the information retrieved:\n",
      "\n",
      "1. **LoRA** (Low-Rank Adaptation) is a method used to adapt large-scale pre-trained language models to specific tasks by injecting trainable rank decomposition matrices into each layer of the Transformer architecture. By implementing LoRA in your applications, you can efficiently switch between different tasks, reduce storage requirements, and save costs by reducing GPU memory requirements and training costs.\n",
      "\n",
      "2. **LongLora** is a framework that extends the context length of large language models efficiently while minimizing accuracy compromise. It incorporates the S2-Attn mechanism during training to approximate standard self-attention patterns and includes components like the Action Units Relation Transformer (ART) and Tampered AU Prediction (TAP) to enhance deep fake detection model generalization.\n",
      "\n",
      "3. **Comparison**: Based on the limited information available, it seems that **LongLora** is considered better than **LoRA**. However, further details on the comparison between LongLora and LoRA are not provided in the retrieved context.\n",
      "\n",
      "If you need more detailed information or have any specific questions, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Explain what is Lora and how I can use it in my applications and saves me money?\"\n",
    "    \"Explain what is LongLora and why it's being used.\"\n",
    "    \"Compare LongLora and LoRA. Which one is better?\"\n",
    ")\n",
    "\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (agenv)",
   "language": "python",
   "name": "agenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
