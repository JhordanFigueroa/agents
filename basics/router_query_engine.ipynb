{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(input_files=[\"./datasets/lora_paper.pdf\"]).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_label: 1\n",
      "file_name: lora_paper.pdf\n",
      "file_path: datasets/lora_paper.pdf\n",
      "file_type: application/pdf\n",
      "file_size: 1609513\n",
      "creation_date: 2024-06-08\n",
      "last_modified_date: 2024-06-08\n",
      "\n",
      "LORA: L OW-RANK ADAPTATION OF LARGE LAN-\n",
      "GUAGE MODELS\n",
      "Edward Hu‚àóYelong Shen‚àóPhillip Wallis Zeyuan Allen-Zhu\n",
      "Yuanzhi Li Shean Wang Lu Wang Weizhu Chen\n",
      "Microsoft Corporation\n",
      "{edwardhu, yeshe, phwallis, zeyuana,\n",
      "yuanzhil, swang, luw, wzchen }@microsoft.com\n",
      "yuanzhil@andrew.cmu.edu\n",
      "(Version 2)\n",
      "ABSTRACT\n",
      "An important paradigm of natural language processing consists of large-scale pre-\n",
      "training on general domain data and adaptation to particular tasks or domains. As\n",
      "we pre-train larger models, full Ô¨Åne-tuning, which retrains all model parameters,\n",
      "becomes less feasible. Using GPT-3 175B as an example ‚Äì deploying indepen-\n",
      "dent instances of Ô¨Åne-tuned models, each with 175B parameters, is prohibitively\n",
      "expensive. We propose Low-RankAdaptation, or LoRA, which freezes the pre-\n",
      "trained model weights and injects trainable rank decomposition matrices into each\n",
      "layer of the Transformer architecture, greatly reducing the number of trainable pa-\n",
      "rameters for downstream tasks. Compared to GPT-3 175B Ô¨Åne-tuned with Adam,\n",
      "LoRA can reduce the number of trainable parameters by 10,000 times and the\n",
      "GPU memory requirement by 3 times. LoRA performs on-par or better than Ô¨Åne-\n",
      "tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite hav-\n",
      "ing fewer trainable parameters, a higher training throughput, and, unlike adapters,\n",
      "no additional inference latency . We also provide an empirical investigation into\n",
      "rank-deÔ¨Åciency in language model adaptation, which sheds light on the efÔ¨Åcacy of\n",
      "LoRA. We release a package that facilitates the integration of LoRA with PyTorch\n",
      "models and provide our implementations and model checkpoints for RoBERTa,\n",
      "DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA .\n",
      "1 I NTRODUCTION\n",
      "Pretrained \n",
      "Weights\n",
      "ùëä‚àà‚Ñùùëë√óùëë\n",
      "xh\n",
      "ùêµ=0\n",
      "ùê¥=ùí©(0,ùúé2)\n",
      "ùëëùëüPretrained \n",
      "Weights\n",
      "ùëä‚àà‚Ñùùëë√óùëë\n",
      "xf(x)\n",
      "ùëë\n",
      "Figure 1: Our reparametriza-\n",
      "tion. We only train AandB.Many applications in natural language processing rely on adapt-\n",
      "ingonelarge-scale, pre-trained language model to multiple down-\n",
      "stream applications. Such adaptation is usually done via Ô¨Åne-tuning ,\n",
      "which updates all the parameters of the pre-trained model. The ma-\n",
      "jor downside of Ô¨Åne-tuning is that the new model contains as many\n",
      "parameters as in the original model. As larger models are trained\n",
      "every few months, this changes from a mere ‚Äúinconvenience‚Äù for\n",
      "GPT-2 (Radford et al., b) or RoBERTa large (Liu et al., 2019) to a\n",
      "critical deployment challenge for GPT-3 (Brown et al., 2020) with\n",
      "175 billion trainable parameters.1\n",
      "Many sought to mitigate this by adapting only some parameters or\n",
      "learning external modules for new tasks. This way, we only need\n",
      "to store and load a small number of task-speciÔ¨Åc parameters in ad-\n",
      "dition to the pre-trained model for each task, greatly boosting the\n",
      "operational efÔ¨Åciency when deployed. However, existing techniques\n",
      "‚àóEqual contribution.\n",
      "0Compared to V1, this draft includes better baselines, experiments on GLUE, and more on adapter latency.\n",
      "1While GPT-3 175B achieves non-trivial performance with few-shot learning, Ô¨Åne-tuning boosts its perfor-\n",
      "mance signiÔ¨Åcantly as shown in Appendix A.\n",
      "1arXiv:2106.09685v2  [cs.CL]  16 Oct 2021\n"
     ]
    }
   ],
   "source": [
    "node_metadata = nodes[0].get_content(metadata_mode=True)\n",
    "print(str(node_metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "###LLM AND EMBEDDING MODEL\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "Settings.embedding = OpenAIEmbedding(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SummaryIndex, VectorStoreIndex\n",
    "\n",
    "summary_index = SummaryIndex(nodes=nodes)\n",
    "vector_index = VectorStoreIndex(nodes=nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATING QUERY ENGINES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_query_engine = summary_index.as_query_engine(\n",
    "    response_model = \"tree_summary\",\n",
    "    use_async=True,\n",
    ")\n",
    "\n",
    "vector_query_engine = vector_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QUERY TOOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "summary_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=summary_query_engine, \n",
    "    description=(\n",
    "        \"Useful for summarization of the lora paper.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "vector_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=vector_query_engine,\n",
    "    description=(\n",
    "        \"Useful for retrieving specific context related to the lora paper.\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#router query engine\n",
    "from llama_index.core.query_engine.router_query_engine import RouterQueryEngine\n",
    "from llama_index.core.selectors import LLMSingleSelector\n",
    "\n",
    "query_engine = RouterQueryEngine(\n",
    "    selector=LLMSingleSelector.from_defaults(),\n",
    "    query_engine_tools=[summary_tool, vector_tool],\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 0: Useful for summarization of the lora paper..\n",
      "\u001b[0mThe LoRA paper introduces a novel adaptation strategy called Low-Rank Adaptation, which aims to address the challenges of fine-tuning large language models for specific tasks. LoRA freezes the pre-trained model weights and introduces trainable rank decomposition matrices into each layer of the Transformer architecture. This approach significantly reduces the number of trainable parameters for downstream tasks, leading to improved efficiency in terms of memory requirements and training throughput. The paper demonstrates that LoRA outperforms traditional fine-tuning methods on various language models like RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters. Additionally, LoRA does not introduce any additional inference latency, making it a promising solution for adapting large language models to specific tasks efficiently.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What is the lora paper about?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(response.source_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 1: The question is asking about specific datasets used in the lora paper, which is more related to retrieving specific context rather than summarization..\n",
      "\u001b[0mThe evaluation datasets used in the LoRA paper include MNLI, STS-B, WikiSQL, SAMSum, E2E NLG Challenge, DART, and WebNLG.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What eval datasets where used in the lora paper?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNLI-\n",
      "ndescribes a subset with ntraining examples. We evaluate with the full validation set. LoRA\n",
      "performs exhibits favorable sample-efÔ¨Åciency compared to other methods, including Ô¨Åne-tuning.\n",
      "To be concrete, let the singular values of Ui‚ä§\n",
      "AUj\n",
      "Bto beœÉ1,œÉ2,¬∑¬∑¬∑,œÉpwherep= min{i,j}. We\n",
      "know that the Projection Metric Ham & Lee (2008) is deÔ¨Åned as:\n",
      "d(Ui\n",
      "A,Uj\n",
      "B) =Óµ™Óµ´Óµ´‚àöp‚àíp‚àë\n",
      "i=1œÉ2\n",
      "i‚àà[0,‚àöp]\n",
      "23\n"
     ]
    }
   ],
   "source": [
    "print(response.source_nodes[0].get_content())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (agenv)",
   "language": "python",
   "name": "agenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
