{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TOOL CALLING WITH AGENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINE TOOLS\n",
    "def add(x: int, y: int) -> int:\n",
    "    \"\"\"Add two numbers together\"\"\"\n",
    "    return x + y\n",
    "\n",
    "def substract(x: int, y: int) -> int:\n",
    "    \"\"\"Substract two numbers\"\"\"\n",
    "    return x - y\n",
    "\n",
    "def multiply(x: int, y: int) -> int:\n",
    "    \"\"\"Multiply two numbers\"\"\"\n",
    "    return x * y\n",
    "\n",
    "def divide(x: int, y: int) -> int:\n",
    "    \"\"\"Divide two numbers\"\"\"\n",
    "    return x / y\n",
    "\n",
    "def get_user_info(username: str) -> str:\n",
    "    \"\"\"Get user information\"\"\"\n",
    "\n",
    "    database = {\n",
    "        \"Antonio\": {\n",
    "            \"name\": \"Antonio Lopez\",\n",
    "            \"age\": 30,\n",
    "            \"email\": \"antonio@example.com\"\n",
    "        }, \n",
    "        \"Nelson\": {\n",
    "            \"name\": \"Nelson Rodriguez\",\n",
    "            \"age\": 25,\n",
    "            \"email\": \"nelson@example.com\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return f\"Username: {username}, Info: {database.get(username.lower(), 'User not found')}\"\n",
    "\n",
    "\n",
    "#CREATE TOOLS FROM PYTHON FUNCTIONS \n",
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "addition_tools = FunctionTool.from_defaults(fn=add)\n",
    "substraction_tools = FunctionTool.from_defaults(fn=substract)\n",
    "multiplication_tools = FunctionTool.from_defaults(fn=multiply)\n",
    "divide_tools = FunctionTool.from_defaults(fn=divide)\n",
    "get_user_info_tools = FunctionTool.from_defaults(fn=get_user_info)\n",
    "\n",
    "tools = [addition_tools, substraction_tools, multiplication_tools, divide_tools, get_user_info_tools]\n",
    "\n",
    "#TEST TOOLS\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: add with args: {\"x\": 2, \"y\": 3}\n",
      "=== Function Output ===\n",
      "5\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "response = llm.predict_and_call(\n",
    "    tools,\n",
    "    \"Add 2 and 3\", \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VECTOR SEARCH WITH METADATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "#read paper\n",
    "documents = SimpleDirectoryReader(input_files=[\"./datasets/lora_paper.pdf\"]).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "nodes = splitter.get_nodes_from_documents(documents=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "###LLM AND EMBEDDING MODEL\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "Settings.embedding = OpenAIEmbedding(model=\"text-embedding-ada-002\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "vector_index = VectorStoreIndex(nodes=nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.vector_stores import MetadataFilters\n",
    "\n",
    "query_engine = vector_index.as_query_engine(\n",
    "    similarity_top_k=3,\n",
    "    filters=MetadataFilters.from_dicts(\n",
    "        [\n",
    "            {\"key\": \"page_label\", \"value\": \"2\"}\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "response = query_engine.query(\"Tell me about the problem statement as explained in page 2\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in response.source_nodes:\n",
    "    print(n.metadata)\n",
    "    print(\"==================\")\n",
    "    print(n.get_text())\n",
    "    print(\"==================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from llama_index.core.vector_stores import FilterCondition\n",
    "from llama_index.core.vector_stores import MetadataFilters\n",
    "\n",
    "def vector_search_query(\n",
    "        query: str, \n",
    "        page_numbers: List[str]\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Conduct a vector search across an index using the following parameters:\n",
    "\n",
    "    query (str): This is the text string you want to embed and search for within the index.\n",
    "    page_numbers (List[str]): This parameter allows you to limit the search to \n",
    "    specific pages. If left empty, the search will encompass all pages in the index. \n",
    "    If page numbers are specified, the search will be filtered to only include those pages.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    metadata_dicts = [\n",
    "        {\"key\": \"page_label\", \"value\": p} for p in page_numbers\n",
    "    ]\n",
    "\n",
    "    query_engine = vector_index.as_query_engine(\n",
    "        similarity_top_k=2,\n",
    "        filters=MetadataFilters.from_dicts(\n",
    "            metadata_dicts,\n",
    "            condition=FilterCondition.OR\n",
    "        )\n",
    "    )\n",
    "\n",
    "    response = query_engine.query(query)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_query_tool = FunctionTool.from_defaults(\n",
    "    fn=vector_search_query,\n",
    "    name=\"vector_search_query_tool\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: vector_search_query_tool with args: {\"query\": \"problem statement\", \"page_numbers\": [\"2\"]}\n",
      "=== Function Output ===\n",
      "We focus on language modeling as our motivating use case, particularly on maximizing conditional probabilities given a task-specific prompt. The scenario involves adapting a pre-trained autoregressive language model to downstream conditional text generation tasks like summarization, machine reading comprehension (MRC), and natural language to SQL (NL2SQL). Each task is defined by a dataset of context-target pairs, where both the context and target are sequences of tokens. For instance, in NL2SQL, the context is a natural language query and the target is the corresponding SQL command; in summarization, the context is an article's content and the target is its summary.\n"
     ]
    }
   ],
   "source": [
    "response=llm.predict_and_call(\n",
    "    [vector_query_tool],\n",
    "    \"Explain the problem statement in page 2\",\n",
    "    verbose=True\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page_label': '2', 'file_name': 'lora_paper.pdf', 'file_path': 'datasets/lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-06-08', 'last_modified_date': '2024-06-08'}\n",
      "==================\n",
      "often introduce inference latency (Houlsby et al., 2019; Rebufﬁ et al., 2017) by extending model\n",
      "depth or reduce the model’s usable sequence length (Li & Liang, 2021; Lester et al., 2021; Ham-\n",
      "bardzumyan et al., 2020; Liu et al., 2021) (Section 3). More importantly, these method often fail to\n",
      "match the ﬁne-tuning baselines, posing a trade-off between efﬁciency and model quality.\n",
      "We take inspiration from Li et al. (2018a); Aghajanyan et al. (2020) which show that the learned\n",
      "over-parametrized models in fact reside on a low intrinsic dimension. We hypothesize that the\n",
      "change in weights during model adaptation also has a low “intrinsic rank”, leading to our proposed\n",
      "Low-RankAdaptation (LoRA) approach. LoRA allows us to train some dense layers in a neural\n",
      "network indirectly by optimizing rank decomposition matrices of the dense layers’ change during\n",
      "adaptation instead, while keeping the pre-trained weights frozen, as shown in Figure 1. Using GPT-3\n",
      "175B as an example, we show that a very low rank (i.e., rin Figure 1 can be one or two) sufﬁces even\n",
      "when the full rank (i.e., d) is as high as 12,288, making LoRA both storage- and compute-efﬁcient.\n",
      "LoRA possesses several key advantages.\n",
      "• A pre-trained model can be shared and used to build many small LoRA modules for dif-\n",
      "ferent tasks. We can freeze the shared model and efﬁciently switch tasks by replacing the\n",
      "matricesAandBin Figure 1, reducing the storage requirement and task-switching over-\n",
      "head signiﬁcantly.\n",
      "• LoRA makes training more efﬁcient and lowers the hardware barrier to entry by up to 3\n",
      "times when using adaptive optimizers since we do not need to calculate the gradients or\n",
      "maintain the optimizer states for most parameters. Instead, we only optimize the injected,\n",
      "much smaller low-rank matrices.\n",
      "• Our simple linear design allows us to merge the trainable matrices with the frozen weights\n",
      "when deployed, introducing no inference latency compared to a fully ﬁne-tuned model, by\n",
      "construction.\n",
      "• LoRA is orthogonal to many prior methods and can be combined with many of them, such\n",
      "as preﬁx-tuning. We provide an example in Appendix E.\n",
      "Terminologies and Conventions We make frequent references to the Transformer architecture\n",
      "and use the conventional terminologies for its dimensions. We call the input and output di-\n",
      "mension size of a Transformer layer dmodel . We useWq,Wk,Wv, andWoto refer to the\n",
      "query/key/value/output projection matrices in the self-attention module. WorW0refers to a pre-\n",
      "trained weight matrix and ∆Wits accumulated gradient update during adaptation. We use rto\n",
      "denote the rank of a LoRA module. We follow the conventions set out by (Vaswani et al., 2017;\n",
      "Brown et al., 2020) and use Adam (Loshchilov & Hutter, 2019; Kingma & Ba, 2017) for model\n",
      "optimization and use a Transformer MLP feedforward dimension dffn= 4×dmodel .\n",
      "2 P ROBLEM STATEMENT\n",
      "While our proposal is agnostic to training objective, we focus on language modeling as our motivat-\n",
      "ing use case. Below is a brief description of the language modeling problem and, in particular, the\n",
      "maximization of conditional probabilities given a task-speciﬁc prompt.\n",
      "Suppose we are given a pre-trained autoregressive language model PΦ(y|x)parametrized by Φ.\n",
      "For instance, PΦ(y|x)can be a generic multi-task learner such as GPT (Radford et al., b; Brown\n",
      "et al., 2020) based on the Transformer architecture (Vaswani et al., 2017). Consider adapting this\n",
      "pre-trained model to downstream conditional text generation tasks, such as summarization, machine\n",
      "reading comprehension (MRC), and natural language to SQL (NL2SQL). Each downstream task is\n",
      "represented by a training dataset of context-target pairs: Z={(xi,yi)}i=1,..,N, where both xiand\n",
      "yiare sequences of tokens. For example, in NL2SQL, xiis a natural language query and yiits\n",
      "corresponding SQL command; for summarization, xiis the content of an article and yiits summary.\n",
      "2\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "for n in response.source_nodes:\n",
    "    print(n.metadata)\n",
    "    print(\"==================\")\n",
    "    print(n.get_text())\n",
    "    print(\"==================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SummaryIndex\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "summary_index = SummaryIndex(nodes=nodes)\n",
    "\n",
    "summary_query_engine_tool = summary_index.as_query_engine(\n",
    "    use_async=True,\n",
    "    response_mode=\"tree_summarize\"\n",
    ")\n",
    "\n",
    "summary_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=summary_query_engine_tool,\n",
    "    name=\"summary_tool\",\n",
    "    description=\"Useful for summarization questions related to the Lora paper.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: summary_tool with args: {\"input\": \"Apply Lora to Transformer by incorporating the Lora module into the Transformer architecture, enabling the model to capture long-range dependencies efficiently. This integration involves modifying the self-attention mechanism in the Transformer to include Lora's attention mechanism for improved performance.\"}\n",
      "=== Function Output ===\n",
      "Incorporating LoRA into the Transformer architecture involves modifying the self-attention mechanism to include LoRA's attention mechanism. This adaptation allows the model to efficiently capture long-range dependencies, leading to improved performance in handling downstream tasks.\n"
     ]
    }
   ],
   "source": [
    "response = llm.predict_and_call(\n",
    "    [summary_tool, vector_query_tool],\n",
    "    \"Summarize how to apply Lora to Transfomer in page 5 in 2 sentences\",\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (agenv)",
   "language": "python",
   "name": "agenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
